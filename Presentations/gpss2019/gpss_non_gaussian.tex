%\documentclass[serif(Seeger and Opper??), mathserif, professionalfont]{beamer} % Remove notes for no notes
\documentclass[serif, mathserif, professionalfont]{beamer} % Remove notes for no notes
%\documentclass[notes]{beamer}       % print frame + notes
%\documentclass[notes=only]{beamer}   % only notes
%\documentclass{beamer}              % only frames
\usepackage{animate}
\usetheme{Prowler}
\usenavigationsymbolstemplate{}
\renewcommand{\newblock}{}
\setlength{\parskip}{1em}

\usepackage{natbib}
\bibliographystyle{apalike}%plainnat}

\usepackage{bm}
\usepackage{xmpmulti}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{palatino}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{relsize}
\usepackage{setspace}
\usepackage{pxfonts}
\usepackage{multicol}
\usepackage{color}
\usepackage{tikz,pgfplots}
\usepackage{comment}
\usepackage{cancel}
\usetikzlibrary{pgfplots.groupplots}
\usetikzlibrary{plotmarks}
\pgfplotsset{compat=newest}
\pgfplotsset{filter discard warning=false}
\usepgfplotslibrary{groupplots}
\definecolor{english}{rgb}{0.0, 0.5, 0.0}
\definecolor{ballblue}{rgb}{0.13, 0.67, 0.8}
\definecolor{MediumSeaGreen}{rgb}{0.235, 0.7, 0.444}
\definecolor{SaddleBrown}{rgb}{0.545, 0.27, 0.075}
\definecolor{SlateBlue}{rgb}{0.415, 0.352, 0.804}
\definecolor{FireBrick}{rgb}{0.698, 0.132, 0.132}


\input{definitions.tex}
\input{notationDef.tex}
\usepackage{xcolor}

\newcommand{\aV}{\mathbf{a}}
\newcommand{\fV}{\mappingFunctionVector}
\newcommand{\yV}{\dataVector}
\newcommand{\noiseVar}{\dataStd^{2}}
\newcommand{\xV}{\inputVector}
\newcommand{\Wmat}{\bm{W}}
\newcommand{\Cmat}{\bm{C}}
\newcommand{\muV}{\bm{\mu}}
\newcommand{\sigmaV}{\bm{\sigma}}
%\newcommand{\likVar}{\beta^{-1}}

%\definecolor{intract}{rgb}{0.227, 0.038, 0.54}
\definecolor{intract}{RGB}{227, 38, 54}
\definecolor{tract}{RGB}{86, 130, 3}

\title{Non-Gaussian likelihoods for Gaussian Processes}
\author{\large Alan Saul}
\institute{{\includegraphics[height=1.1cm]{../diagrams/prowler-primary-logo-lbg.png}}}
\date{}

\defbeamertemplate*{title page}{customized}[1][]
{
	\centering
    %\setbeamercolor{coloredboxstuff}{fg=yellow,bg=blue}
  	\begin{beamercolorbox}[wd=\paperwidth,ht=8.8ex,dp=5.8ex]{frametitle}
   		 \centering  \usebeamerfont{title}\inserttitle
     \end{beamercolorbox}
 
   %\vspace{1.4cm}
   \bigskip
   \insertauthor\par
   \insertinstitute\par
   \insertdate\par
 
}

\begin{document}
%\frame{
%\begin{center}
    %{\huge Non-Gaussian likelihoods for Gaussian Processes}\\
    %\vspace{2em}
    %Alan Saul\\
    %University of Sheffield\\
    %\vspace{2em}
    %%\includegraphics[width=0.4\textwidth]{../../../logo_sheff}
%\end{center}
%}
\addtobeamertemplate{frametitle}{}{%
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=north east,yshift=2pt] at (current page.north east) {\includegraphics[height=0.8cm]{../diagrams/prowler-icon-agent.png}};
\end{tikzpicture}}

\frame{\titlepage}


\frame{
\frametitle{Outline}
    \tableofcontents%[currentsection]%,hideallsubsections,subsubsectionstyle=hide]
}
\note{
    Motivation:
        What is a likelihood?
        non-Gaussian likelihoods, examples, 
        Bernoulli, Student-T, Poisson
        True posterior approximation not possible, all the following methods look at approximating this true posterior with a Gaussian distribution in some way.
        Once we have a Gaussian approximation to the posterior, we can use it as normal, for example making predictions etc.
        Many ways in which we could approximate the distribution, approximate around shape of a single mode, approximate overall density, should we care more about capturing density of leaving it out if we have to choose?
        Show an overview of how the different methods might approximate different distributions
    Laplace approximation
        Find the mode of the true posterior distribution
        Do a taylor expansion around this point, which finds the curvature around this point
        Approximate with a Gaussian with this mean and the same curvature
    KL method
        We say we have a Gaussian distribution, whos parameters we can tweak to match the shape of the posterior
        We define a similarity measure between our Gaussian distribution, and the true posterior, this is the KL.
        KL is the average additional amount of information required to specify the values of $f$ as a result of using an approximate distribution $q(f)$ instead of the true distribution, $p(f|y)$.
        It is not a symmetrical quantity and is always zero or positive (we must provide some additional information, or no information at all).
        Show some examples of how the KL changes for different likelihoods, maybe a convolution
        We can't compute the KL divergence between the true posterior and the approximate posterior, as it requires integrating over a quantity we can't compute directly?
        The trick is to try and minimize the $KL[q(f|y)||p(f|y)]$, by forming a lower bound on the marginal likelihood which we will in turn maximise
        This needs lots of figures!
        Leads to distributions $q(f)$ that avoid regions of $p(f|y)$ that have low density (heavily penalized for putting density where there is little)
    Expectation propagation
        This is a more algorithmic approach to making the approximation. Shown to be the most effective method for the classification likelihood.
        Looks at approximating the true posterior with a product of site-approximations (Gaussian distributions).
        These site distributions are -not- Gaussian approximations to each likelihood component, but overall make the marginal likelihood similar to true marginal
        Iterative procedure, give outline
        Moment match - set the mean and variances of the two distributions to be equal
        Although for EP the KL divergence is only for each of the marginals individually $KL[p(f_i|y_i)||q(fi)]$, not the full KL divergence $KL[p(f|y)||q(f)]$, this KL divergence is minimized when density is assigned by $q(fi)$ to regions where $p(fi|yi)$ has density (heavily penalized for not putting density where there is lots)
    Overall properties, when to use which?
        Laplace approximation 
            Pros - Very fast
            Cons - Poor approximation if the mode does not well describe the posterior, for example Bernoulli likelihood (probit)
                 - As a result it can assign lots of density to areas where there is none
            When - When the posterior is well characterized by its mode, for example Poisson.
        KL method
            Pros - Can be relatively quick, and lends it self to sparse approximations
                 - Principled in that it we are directly optimizing a measure of similarity between an approximation and true distribution
            Cons - 
            When - When the we 
        EP method
            Pros - Very effective for certain likelihoods (classification)
            Cons - Slow though possible to extend to sparse case. 
                 - Convergence issues for certain likelihoods
                 - Must be able to match moments
            When -
        MCMC methods
            If accuracy is key, it is also possible to use a range of different sampling techniques
            These have a tendency to be very slow though in the limit of infinite samples, they are theoretically exact.
        \cite{Approximations for Binary Gaussian Process Classification}
    Bonus
        Heteroscedastic likelihoods
}

\frame{
    \frametitle{GP regression - recap so far}
    \begin{overprint}
        Model the observations as a distorted version of the process $\fV_{i} = f(\xV_{i})$:
    \begin{equation*}
        \yV_{i} \sim \gaussianSamp{f(\xV_{i})}{\noiseVar}
        %\yV_{i} \sim \gaussianDist{\yV_{i}}{f(\xV_{i})}{\noiseVar}
    \end{equation*}
    $f$ is a non-linear function, in our case we assume it is latent, and is assigned a Gaussian process prior.
    \end{overprint}
    \begin{overprint}
        \centerline{\includegraphics[width=1\textwidth]{../diagrams/PosteriorGP.pdf}}
    \end{overprint}
}

\frame{
    \frametitle{GP regression setting}
    So far we have assumed that the latent values, $\fV$, have been corrupted by Gaussian noise.
    Everything remains analytically tractable.
    \begin{align*}
        \text{Gaussian Prior:} & & \fV \sim \mathcal{GP}(\zerosVector, \Kff) = p(\fV)\\
        \text{Gaussian likelihood:} & & \yV \sim \gaussianSamp{\fV}{\noiseVar\eye} = \prod^{\numData}_{i=1} p(\yV_{i}|\fV_{i})\\
        \text{Gaussian posterior:} & & p(\fV|\yV) \propto \gaussianDist{\yV}{\fV}{\noiseVar\eye}\gaussianDist{\fV}{\zerosVector}{\Kff}
    \end{align*}
}

\section{Motivation}
\frame{
\frametitle{Outline}
    \tableofcontents[currentsection,
    currentsubsection,
    subsectionstyle=show/shaded]
}

\frame{
    \frametitle{Motivation}
    \begin{itemize}
        \item You have been given some data you wish to model.
        \item You believe that the observations are connected through some underlying unknown function.
        \item You know from your understanding of the data generation process, that the observations are not Gaussian.
        \item You still want to learn, as best as possible, what is the unknown function being used, and make predictions.
    \end{itemize}
    \begin{overprint}
        \centerline{\includegraphics[width=.99\textwidth]{../diagrams/datatypes.pdf}}
    \end{overprint}
}

\frame{
    \frametitle{Likelihood}
    \begin{itemize}
        \uncover<+->{\item $p(\yV|\fV)$ is the probability that we would see some random variables, $\yV$, if we knew the latent function values $\fV$, which act as parameters.}
        \uncover<+->{\item Given the observed values for $\yV$ are fixed, it can also be seen as the likelihood that some latent function values, $\fV$, would give rise to the observed values of $\yV$. Note this is a \emph{function} of $\fV$, and doesn't integrate to $1$ in $\fV$.}
        \uncover<+->{\item Often observations aren't observed by simple Gaussian corruptions of the underlying latent function, $\fV$.}
        %\item So far assumed that the distortion of the underlying latent function, $\fV$, that gives rise to the observed data, $\yV$, is independent and normally distributed.
        \uncover<+->{\item In the case of count data, binary data, etc, we need to choose a different likelihood function.}
    \end{itemize}
}

\frame{
    \frametitle{Likelihood}
    \centering $p(y|f)$ as a function of y, with fixed f
    \begin{overprint}
        \centerline{\includegraphics[width=.99\textwidth]{../diagrams/all_likelihoods.pdf}}
    \end{overprint}
}

\frame{
    \frametitle{Likelihood}
    \centering $p(y|f)$ as a function of f, with fixed y
    \begin{overprint}
        \centerline{\includegraphics[width=.99\textwidth]{../diagrams/all_actual_likelihoods.pdf}}
    \end{overprint}
}
% ON THE BOARD ILLUSTRATE THE STUDENT-T

\frame{
    \frametitle{Binary example}
    \begin{itemize}
        \item Binary outcomes for $\yV_{i}$, $\yV_{i} \in [0,1]$.
        \item Model the probability of $\yV_{i} = 1$ with transformation of GP, with Bernoulli likelihood.
        \item Probability of $1$ must be between $0$ and $1$, thus use squashing transformation, $\lambda(\fV_{i}) = \Phi(\fV_{i})$.
    \end{itemize}
    \begin{align*}
        p(\yV_{i}|\lambda(\fV_{i})) = \left\{
            \begin{array}{ll}
                \lambda(\fV_{i}), & \text{if $\yV_{i} = 1$}\\
                1-\lambda(\fV_{i}), & \text{if $\yV_{i} = 0$}
            \end{array}
        \right.
    \end{align*}
    %\begin{align*}
        %p(\yV_{i}|\lambda(\fV_{i})) = \left\{
            %\begin{array}{ll}
                %1, & \text{with probability $\lambda(\fV_{i})$.}\\
                %0, & \text{with probability $1-\lambda(\fV_{i})$.}
            %\end{array}
        %\right.
    %\end{align*}
    %\begin{align*}
        %\yV_{i} = \left\{
            %\begin{array}{ll}
                %1, & \text{with probability $\lambda(\fV_{i})$.}\\
                %0, & \text{with probability $1-\lambda(\fV_{i})$.}
            %\end{array}
        %\right.
    %\end{align*}
    \centerline{\includegraphics[width=1.0\textwidth]{../diagrams/BernoulliPosterior.pdf}}
}

\frame{
    \frametitle{Count data example}
    \begin{overprint}
    \begin{itemize}
        \item Non-negative and discrete values only for $\yV_{i}$, $\yV_{i} \in \mathbb{N}$.
        \item Model the \emph{rate} or \emph{intensity}, $\lambda$, of events with a transformation of a Gaussian process.
        \item Rate parameter must remain positive, use transformation to maintain positiveness $\lambda(\fV_{i}) = \exp(\fV_{i})$ or $\lambda(\fV_{i}) = \fV_{i}^2$
    \end{itemize}
    \begin{align*}
        \yV_{i} \sim \text{Poisson}(\yV_{i}|\lambda_{i} = \lambda(\fV_{i}))\quad\quad
        \text{Poisson}(\yV_{i}|\lambda_{i}) = \frac{\lambda_{i}^{\yV_{i}}}{!\yV_{i}}e^{-\lambda_{i}}
    \end{align*}
\end{overprint}
\begin{overprint}
    \centerline{\includegraphics[width=.8\textwidth]{../diagrams/PoissonPosterior.pdf}}
\end{overprint}
}

\frame{
    \frametitle{Application example}
\begin{columns}[onlytextwidth]
    \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item Chicago crime counts.
            \item Same Poisson likelihood.
            \item 2D-input to kernel.
        \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
        \begin{overprint}
            \centerline{\includegraphics[width=\textwidth]{../diagrams/crime_func_f_fast.pdf}}
        \end{overprint}
    \end{column}
\end{columns}
}

%\frame{
    %\frametitle{Application example}
    %MORE COMPLEX MOTIVATING APPLICATION. MNIST MULTICLASS
%}


\section{Non-Gaussian posteriors}
\frame{
\frametitle{Outline}
    \tableofcontents[currentsection,
    currentsubsection,
    subsectionstyle=show/shaded]
}

\frame{
    \frametitle{Non-Gaussian posteriors}
    \begin{itemize}
        \item Exact computation of posterior is no longer analytically tractable due to non-conjugate Gaussian process prior to non-Gaussian likelihood, $p(\yV|\fV)$.
    \end{itemize}
    \begin{equation*}
        {\color{intract}p(\fV|\yV)} = \frac{p(\fV)\prod^{\numData}_{i=1}p(\yV_{i}|\fV_{i})}{\color{intract}\int p(\fV)\prod^{\numData}_{i=1}p(\yV_{i}|\fV_{i})\,d\fV}
    \end{equation*}

    Why is it so difficult?
}

\frame{
    \frametitle{Non-Gaussian posteriors illustrated}
    \begin{itemize}
        \item Consider one observation, $y_{1} = 1$, at input $x_{1}$.
        \item Can normalise easily with numerical integration, $\int p(y_1=1|\lambda(f_1))p(f_1) df_1$.
    \end{itemize}
    \begin{overprint}
        \onslide<1>\centerline{\includegraphics[width=0.9\textwidth]{../diagrams/1d_gp_prior_samples.pdf}}
        \onslide<2>\centerline{\includegraphics[width=0.9\textwidth]{../diagrams/1d_squashed_gp_prior_samples.pdf}}
        \onslide<3>\centerline{\includegraphics[width=0.9\textwidth]{../diagrams/1d_gp_post_samples.pdf}}
    \end{overprint}
}
% So I've told you that its difficult to do this integration, but I now want to illustrate why this is the case. Often when people talk about the posterior they only really illustrate the posterior in one dimension, I think its useful to understand what is really happening when we get to this problem with Gaussian processes, that is we need to do this integral in a far higher dimension.

\frame{
    \frametitle{Non-Gaussian posteriors illustrated}
    \begin{overprint}
        \centerline{\includegraphics[width=0.9\textwidth]{../diagrams/1d_marginals.pdf}}
    \end{overprint}
}

\frame{
    \frametitle{Non-Gaussian posteriors illustrated}
    \begin{itemize}
        \item Now two observations, $y_{1} = 1$ and $y_{2} = 1$ at $x_{1}$ and $x_{2}$
        \item Need to calculate the joint posterior, $p(\fV|\yV) = p(f_{1}, f_{2}| y_{1}=1, y_{2}=1)$.
        \item Requires 2D integral $\int \int p(y_1=1,y_2=1|\lambda(f_1),\lambda(f_2))p(f_1, f_2) df_1 df_2$.
    \end{itemize}
    \begin{overprint}
        \onslide<1>\centerline{\includegraphics[width=0.8\textwidth]{../diagrams/2d_gp_prior_samples.pdf}}
        \onslide<2>\centerline{\includegraphics[width=0.8\textwidth]{../diagrams/2d_squashed_gp_prior_samples.pdf}}
        \onslide<3>\centerline{\includegraphics[width=0.8\textwidth]{../diagrams/2d_gp_post_samples.pdf}}
    \end{overprint}
}

\frame{
    \frametitle{Non-Gaussian posteriors illustrated}
    \begin{itemize}
        \item To find the true posterior values, we need to perform a two dimensional integral.
        \item Still possible, but things are getting more difficult quickly.
    \end{itemize}
    \begin{overprint}
        \onslide<1>\centerline{\includegraphics[width=0.8\textwidth]{../diagrams/2d_joint_prior.pdf}}
        \onslide<2>\centerline{\includegraphics[width=0.8\textwidth]{../diagrams/2d_joint_likelihood.pdf}}
        \onslide<3>\centerline{\includegraphics[width=0.8\textwidth]{../diagrams/2d_joint_posterior.pdf}}
    \end{overprint}
}

\frame{
   \frametitle{Approaches to handling non-Gaussian posteriors}
   Generally fall into two areas:
   \begin{itemize}
        \item Sampling methods that obtain samples of the posterior.
        \item Approximation of the posterior with something of known form.
   \end{itemize}

   Today we will focus on the latter.\\~\\

    \centerline{\includegraphics[width=0.3\textwidth]{../diagrams/2d_joint_posterior_no_marginals.pdf}
                \includegraphics[width=0.3\textwidth]{../diagrams/2d_joint_posterior_samples.pdf}
                \includegraphics[width=0.31\textwidth]{../diagrams/joint_kl_no_marginals.pdf}}
}

\frame{
    \frametitle{Non-Gaussian posterior approximation}
    \begin{itemize}
        \item Various methods to make a Gaussian approximation, ${\color{intract}p(\fV|\yV)} \approx q(\fV) = \gaussianDist{\fV}{\mathbf{\mu}=?}{\Cmat=?}$.
        \item Only need to obtain an approximate posterior at the training locations.
        \item At test locations, the data only effects their probabily via the posterior at these locations.
    \end{itemize}
    \begin{align*}
        p(\fV, \fV^{*}|\xV^{*}, \xV, \yV) &= p(\fV^*|\fV, \xV^{*})p(\fV|\xV, \yV)\\
        %&\propto p(\fV^{*}|\fV, \xV^{*})p(\yV|\fV, \xV)p(\fV|\xV)
    \end{align*}
}

\frame{
    \frametitle{Why do we want the posterior anyway?}
    True posterior, posterior approximation, or samples are needed to make predictions at new locations, $\xV^{*}$.
    \begin{align*}
        p(\fV^{*}|\xV^{*}, \xV, \yV) &= \int p(\fV^{*}|\fV, \xV^{*}){\color{intract}{p(\fV|\yV, \xV)}} d\fV\\
        q(\fV^{*}|\xV^{*}, \xV, \yV) &= \int p(\fV^{*}|\fV, \xV^{*})q(\fV|\xV) d\fV
    \end{align*}
}

\section{Approximate methods}
\frame{
\frametitle{Outline}
    \tableofcontents[currentsection,
    currentsubsection,
    subsectionstyle=show/shaded]
}

\frame{
    \frametitle{Methods overview}
    Given choice of Gaussian approximation of posterior. How do we choose the parameter values $\mu$ and $\Cmat$?

    There a number of different methods in which to choose how to set the parameters of our Gaussian approximation.
}

\frame{
    \frametitle{Parameters effect - mean}
    \animategraphics[loop,width=1\linewidth]{20}{../diagrams/animations/mean-change-frame-}{0}{199}
}

\frame{
    \frametitle{Parameters effect - variance}
    \animategraphics[loop,width=1\linewidth]{20}{../diagrams/animations/L-change-frame-}{0}{199}
}

\frame{
    \frametitle{How to choose the parameters?}
    Two approaches that we might take:
    \begin{itemize}
        \item Is to match the mean and variance at some point, for example the mode.
        \item Attempt to minimise some divergence measure between the approximate distribution and the true distribution.
    \end{itemize}

    \begin{itemize}
        \item Laplace takes the former
        \item Variational bayes takes the latter
        \item EP kind of takes the latter
    \end{itemize}
}

\subsection{Laplace approximation}
\frame{
    \frametitle{Outline}
    \tableofcontents[currentsection,
    currentsubsection,
    subsectionstyle=show/shaded]
}

\frame{
    \frametitle{Laplace approximation}
    Task: for some generic random variable, $f$, and data, $y$, find a good approximation to difficult to compute posterior distribution, $\color{intract}p(f|y)$.\\~\\
    Laplace approach: fit a Gaussian by matching the curvature at the modal point of the posterior.
    \begin{itemize}
        \item Use a second-order taylor expansion around the mode of the log-posterior.
        \item Use the expansion to find an equivalent Gaussian in the probability space.
    \end{itemize}
}

\frame{
    \frametitle{Laplace approximation}
    \begin{itemize}
        \item Log of a Gaussian distribution, $q(\fV) = \gaussianDist{\fV}{\mu}{\Cmat}$, is a quadratic function of $\fV$.
        \item A second-order taylor expansion is an approximation of a function using only quadratic terms.
        \item Laplace approximation expands the un-normalised posterior, and then uses it to set the linear and quadratic terms of the log $q(\fV)$.
        \item The first and second derivatives of the form of the log-posterior, at the mode, will match the derivatives of the approximate Gaussian at this same point.
    \end{itemize}
}

\frame{
    \frametitle{Second-order taylor expansion}
    \begin{minipage}[t][3cm][t]{\textwidth}
        \begin{align*}
            \only<1>{p(\fV|\yV) &= \frac{1}{{\color{intract}Z}}{\color{SlateBlue}h(\fV)}\\}%
            \only<2-3>{\log p(\fV|\yV) &= \log \frac{1}{{\color{intract}Z}} + {\color{SlateBlue}\log h(\fV)}\\}%
            \only<3-4>{&\approx \log \frac{1}{{\color{intract}Z}} + {\color{black}\log h(\aV) + \frac{d\log h(\aV)}{d\aV}(\fV - \aV)}\\%
            &{\color{black}+ \frac{1}{2}(\fV - \aV)^\top\frac{d^{2}\log h(\aV)}{d\aV^{2}}(\fV - \aV) + \cdots}}
            \only<5>{\log p(\fV|\yV) &\approx \log \frac{1}{{\color{intract}Z}} + {\color{orange}\log h(\hat{\fV}) + \frac{d\log h(\hat{\fV})}{d\hat{\fV}}(\fV - \hat{\fV})}\\}%
            \only<6>{\log p(\fV|\yV) &\approx \log \frac{1}{{\color{intract}Z}} + {\color{MediumSeaGreen}\log h(\hat{\fV}) + \frac{d\log h(\hat{\fV})}{d\hat{\fV}}(\fV - \hat{\fV})}\\%
            &{\color{MediumSeaGreen}+ \frac{1}{2}(\fV - \hat{\fV})^\top\frac{d^{2}\log h(\hat{\fV})}{d\hat{\fV}^{2}}(\fV - \hat{\fV})}}
            \only<7>{\log p(\fV|\yV) &\approx \log \frac{1}{{\color{intract}Z}} + {\color{FireBrick}\log h(\hat{\fV}) + \frac{d\log h(\hat{\fV})}{d\hat{\fV}}(\fV - \hat{\fV})}\\%
            &{\color{FireBrick}+ \frac{1}{2}(\fV - \hat{\fV})^\top\frac{d^{2}\log h(\hat{\fV})}{d\hat{\fV}^{2}}(\fV - \hat{\fV}) + \cdots}\\}%
            \only<8>{\log p(\fV|\yV) &\approx \log \frac{1}{{\color{intract}Z}} + {\color{MediumSeaGreen}\log h(\hat{\fV}) + \frac{d\log h(\hat{\fV})}{d\hat{\fV}}(\fV - \hat{\fV})}\\%
            &{\color{MediumSeaGreen}+ \frac{1}{2}(\fV - \hat{\fV})^\top\frac{d^{2}\log h(\hat{\fV})}{d\hat{\fV}^{2}}(\fV - \hat{\fV})}\\}%
            \only<9-10>{p(\fV|\yV) &\approx \frac{1}{{\color{intract}Z}}{\color{SaddleBrown}h(\hat{\fV})\exp\left\{-\frac{1}{2}(\fV - \hat{\fV})^{\top}\left(-\frac{d^{2}\log h(\hat{\fV})}{d\hat{\fV}^{2}}\right)(\fV - \hat{\fV})\right\}}\\}%
            \only<10>{&= {\color{MediumSeaGreen}\gaussianDist{\fV}{\hat{\fV}}{\left(-\frac{d^{2}\log h(\hat{\fV})}{d\hat{\fV}^{2}}\right)^{-1}}}}%
        \end{align*}
        \only<1>{{\center In our case: $h(\fV) = p(\yV|\fV)p(\fV)$}\\}%
    \end{minipage}
    \begin{minipage}[t][2cm][t]{\textwidth}
        \only<1>{\centering \includegraphics[height=3.5cm,keepaspectratio]{../diagrams/taylor/hf.pdf}}%
        \only<2>{\centering \includegraphics[height=3.5cm,keepaspectratio]{../diagrams/taylor/log_hf.pdf}}%
        \only<5>{\centering \includegraphics[height=3.5cm,keepaspectratio]{../diagrams/taylor/log_hf_taylor_1.pdf}}%
        \only<6>{\centering \includegraphics[height=3.5cm,keepaspectratio]{../diagrams/taylor/log_hf_taylor_2.pdf}}%
        \only<7>{\centering \includegraphics[height=3.5cm,keepaspectratio]{../diagrams/taylor/log_hf_taylor_3.pdf}}%
        %\only<8>{\centering \includegraphics[height=3.5cm,keepaspectratio]{../diagrams/taylor/log_hf_taylor_4.pdf}}%
        \only<8>{\centering \includegraphics[height=3.5cm,keepaspectratio]{../diagrams/taylor/log_hf_taylor_2_only.pdf}}%
        \only<9>{\centering \includegraphics[height=3.5cm,keepaspectratio]{../diagrams/taylor/exp_approx.pdf}}%
        \only<10>{\centering \includegraphics[height=3.5cm,keepaspectratio]{../diagrams/taylor/norm_approx.pdf}}%
        \only<4>{%
            {\centering Want to make the expansion around the mode, $\hat{\fV}$:}
            \begin{align*}
                \left.\frac{d\log h(\aV)}{d\aV}\right|_{\aV=\hat{\fV}} = \zerosVector
            \end{align*}
            }%
    \end{minipage}
    }

%\frame{
    %\frametitle{Second-order taylor expansion}
    %\begin{align*}
        %\uncover<+->{p(\fV|\yV) &= \frac{1}{{\color{intract}Z}}h(\fV)\\}
        %\uncover<+->{\log p(\fV|\yV) &= \log \frac{1}{{\color{intract}Z}} + \log h(\fV)\\}
        %\uncover<+->{&\approx \log \frac{1}{{\color{intract}Z}} + \log h(\aV) + \frac{d\log h(\aV)}{d\aV}(\fV - \aV)\\
        %&+ \frac{1}{2}(\fV - \aV)^\top\frac{d^{2}\log h(\aV)}{d\aV^{2}}(\fV - \aV)}
    %\end{align*}
    %\uncover<+->{
    %In our case we want to make our expansion around the mode, $\hat{\fV}$:
    %\begin{align*}
        %\left.\frac{d\log h(\aV)}{d\aV}\right|_{\aV=\hat{\fV}} = \zerosVector
    %\end{align*}
%}
%}

%\frame{%
    %\frametitle{Second-order taylor expansion}
    %\begin{align*}
        %\uncover<+->{\log p(\fV|\yV) &\approx \log \frac{1}{{\color{intract}Z}} + \log h(\hat{\fV}) + \frac{d\log h(\hat{\fV})}{d\hat{\fV}}(\fV - \hat{\fV})\\
        %&+ \frac{1}{2}(\fV - \hat{\fV})^\top\frac{d^{2}\log h(\hat{\fV})}{d\hat{\fV}^{2}}(\fV - \hat{\fV})\\}
        %\uncover<+->{p(\fV|\yV) &= \frac{1}{{\color{intract}Z}}h(\hat{\fV})\exp\left\{-\frac{1}{2}(\fV - \hat{\fV})^{\top}\left(-\frac{d^{2}\log h(\hat{\fV})}{d\hat{\fV}^{2}}\right)(\fV - \hat{\fV})\right\}\\}
        %\uncover<+->{&= \gaussianDist{\fV}{\hat{\fV}}{\left(-\frac{d^{2}\log h(\hat{\fV})}{d\hat{\fV}^{2}}\right)^{-1}}}
    %\end{align*}
%}

\frame{%
    \frametitle{Laplace appoximation for Gaussian processes}
    In our case, $h(\fV) = p(\yV|\fV)p(\fV)$, so we need to evaluate
    \begin{align*}
        -\frac{d^{2}\log h(\hat{\fV})}{d\hat{\fV}^{2}} &= -\frac{d^{2}(\log p(\yV|\hat{\fV}) + \log p(\hat{\fV}))}{d\hat{\fV}^{2}}\\
        &= -\frac{d^{2}\log p(\yV|\hat{\fV})}{d\hat{\fV}^{2}} + \mathbf{K}^{-1}\\
        &\triangleq \Wmat + \mathbf{K}^{-1}
    \end{align*}
    giving a posterior approximation:
    \begin{align*}
        p(\fV|\yV) \approx q(\fV) = \gaussianDist{\fV}{\hat{\fV}}{\left(\Wmat + \mathbf{K}^{-1}\right)^{-1}}
    \end{align*}
    }

\frame{
    \frametitle{Laplace approximation - algorithm overview}
    \begin{itemize}
        \item Find the mode, $\hat{\fV}$ of the true log posterior, via Newton's method.
        \item Use second-order Taylor expansion around this modal value.
        \item Form Gaussian approximation setting the mean equal to the posterior mode, $\hat{\fV}$, and matching the curvature.
        \item ${\color{intract}p(\fV|\yV)} \approx q(\fV|\muV,\Cmat) = \gaussianDist{\fV}{\hat{\fV}}{(\mathbf{K}^{-1} + \Wmat)^{-1}}$
        \item $\Wmat \triangleq -\frac{d^{2}\log p(\yV|\hat{\fV})}{d\hat{\fV}^{2}}$.
        \item For factorizing likelihoods (most), $\Wmat$ is diagonal.
    \end{itemize}
}
\note{The actual optimization is in log space, but here we show the result in the distribution space}

\frame{
    \frametitle{Visualization of Laplace}
    %\animategraphics[controls]{1}{../diagrams/laplace-anim}{0}{6}
    \centering
    \multiinclude[<+>][format=pdf,graphics={scale=0.6}]{../diagrams/animations/laplace-anim-log-Poisson}
}
\frame{
    \frametitle{Visualization of Laplace}
    %\animategraphics[controls]{1}{../diagrams/laplace-anim}{0}{6}
    \centering
    \multiinclude[<+>][format=pdf,graphics={scale=0.6}]{../diagrams/animations/laplace-anim-full-Poisson}
}

\frame{
    \frametitle{Visualise of Laplace - Bernoulli}
    \centering
    \multiinclude[<+>][format=pdf,graphics={scale=0.6}]{../diagrams/animations/laplace-anim-full-Bernoulli}
}

\subsection{Variational bayes}
\frame{
\frametitle{Outline}
    \tableofcontents[currentsection,
    currentsubsection,
    subsectionstyle=show/shaded]
}

\frame{
    \frametitle{Variational Bayes (VB)}
    Task: for some generic random variable, $z$, and data, $y$, find a good approximation to difficult to compute posterior distribution, $\color{intract}p(z|y)$.\\~\\
    VB approach: minimise a divergence measure between an approximate posterior, $\color{tract}q(z)$ and true posterior, $\color{intract}p(z|y)$.
    \begin{itemize}
        \item KL divergence, $\color{intract}\KL{q(z)}{p(z|y)}$.
        \item Minimize this with respect to parameters of $\color{tract}q(z)$.
    \end{itemize}
}
% previously tried to approximate the posterior using a local point. Really want something that more globally captures the characteristics of the non-Gaussian posterior
% why KL? What about others? Alpha, F?

\frame{
    \frametitle{KL divergence}
    \begin{itemize}
        \item General for any two distributions $q(\xV)$ and $p(\xV)$.
        %\item $\KL{q(\xV)}{p(\xV)}$ is the average additional amount of information required to specify the values of $\xV$ as a result of using an approximate distribution $q(\xV)$ instead of the true distribution, $p(\xV)$.
        \item $\KL{q(\xV)}{p(\xV)}$ is the average additional amount of information lost when $p(\xV)$ is used to approximate $q(\xV)$. It's a measure of divergence of one distribution to another.
        %\item in information theory it is ued to measure the differences in information contained within the two distributions, and hence how much "suprise" there is of seeing a particular value.
        \item $\KL{q(\xV)}{p(\xV)} = \expectationDist{\log \frac{q(\xV)}{p(\xV)}}{q(\xV)}$
        \item Always 0 or positive, not symmetric.
        \item Lets look at how it changes with response to changes in the approximating distribution.
    \end{itemize}
}

\frame{
    \frametitle{KL varying mean}
    %\animategraphics[controls]{1}{../diagrams/KL_gaussian_mu}{0}{6}
    %\animategraphics[loop,width=1\linewidth]{12}{../diagrams/KL_gaussian_mu-}{0}{4}
    \multiinclude[<+>][format=pdf,graphics={scale=0.4}]{../diagrams/KL_gaussian_mu}
}

\frame{
    \frametitle{KL varying variance}
    %\animategraphics[controls]{1}{../diagrams/KL_gaussian_mu}{0}{6}
    %\animategraphics[loop,width=1\linewidth]{12}{../diagrams/KL_gaussian_var-}{0}{4}
    \multiinclude[<+>][format=pdf,graphics={scale=0.4}]{../diagrams/KL_gaussian_var}
}

\frame{
    \frametitle{Variational Bayes}
    Don't have access to or can't compute for computational reasons: $\color{intract}p(z|y)$ or $\color{intract}p(y)$, and hence $\color{intract}\KL{q(z)}{p(z|y)}$\\~\\
    How can we minimize something we can't compute?
    \begin{itemize}
        \item Can compute $q(z)$ and $p(y|z)$ for any $z$. 
        \item $q(z)$ is parameterised by `variational parameters'.
        \item True posterior using Bayes rule, ${\color{intract}p(z|y)} = \frac{p(y|z)p(z)}{{\color{intract}p(y)}}$.
        \item $\color{intract}p(y)$ doesn't change when variational parameters are changed.
    \end{itemize}
}

%\frame{
    %\frametitle{Variational Bayes - Derivation}
    %\begin{align*}
        %\color{intract}\KL{q(z)}{p(z|y)} &= \expectationDist{\log \frac{q(z)}{{\color{intract}p(z|y)}}}{q(z)}\\
        %&= \expectationDist{\log \frac{q(z)}{p(z)}p(y|z)p(y)}{q(z)}\\
        %&= \expectationDist{\log \frac{q(z)}{p(z)} - \log p(y|z) + {\color{intract}\log p(y)}}{q(z)}\\
        %&= \KL{q(z)}{p(z)} - \expectationDist{\log p(y|z)}{q(z)} + {\color{intract}\log p(y)}\\
        %{\color{intract}\log p(y)} &= {\color{tract}\expectationDist{\log p(y|z)}{q(z)}} - {\color{tract}\KL{q(z)}{p(z)}} + {\color{intract}\KL{q(z)}{p(z|y)}}
    %\end{align*}
%}

\frame{
    \frametitle{Variational Bayes - Derivation}
    \begin{align*}
        \uncover<+->{&\color{intract}\KL{q(z)}{p(z|y)}\\}
        \uncover<+->{&= \int q(z)\left[\log \frac{q(z)}{{\color{intract}p(z|y)}}\right] dz\\}
        %&= \int q(z)\left[\log \frac{q(z)p(y|z)p(y)}{p(z)}\right] dz\\
        \uncover<+->{&= \int q(z)\left[\log \frac{q(z)}{p(z)} - \log p(y|z) + {\color{intract}\log p(y)}\right] dz\\}
        \uncover<+->{&= {\color{tract}\KL{q(z)}{p(z)}} - {\color{tract}\int q(z)\left[\log p(y|z)\right] dz} + {\color{intract}\log p(y)}\\}
        \uncover<+->{{\color{intract}\log p(y)} &= {\color{tract}\int q(z)\left[\log p(y|z)\right] dz} - {\color{tract}\KL{q(z)}{p(z)}} + {\color{intract}\KL{q(z)}{p(z|y)}}\\}
    \end{align*}
}
\frame{
    \frametitle{Variational Bayes - Derivation}
    \begin{align*}
        {\color{intract}\log p(y)} &= {\color{tract}\int q(z)\left[\log p(y|z)\right] dz} - {\color{tract}\KL{q(z)}{p(z)}} + {\color{intract}\KL{q(z)}{p(z|y)}}\\
        &\geq {\color{tract} \int q(z)\left[\log p(y|z)\right] dz} - {\color{tract}\KL{q(z)}{p(z)}}
    \end{align*}
    \begin{itemize}
        \item Tractable terms give lower bound on $\log p(y)$ as $\color{intract}\KL{q(z)}{p(z|y)}$ always positive.
        \item Adjust variational parameters of $q(z)$ to make tractable terms as large as possible, thus $\color{intract}\KL{q(z)}{p(z|y)}$ as small as possible.
    \end{itemize}
}

\frame{
    \frametitle{VB optimisation illustration}
    \animategraphics[loop,width=1\linewidth]{15}{../diagrams/animations/vb-frame-}{0}{39}
}

%\frame{
    %\frametitle{Variational Bayes derivation}
    %\begin{itemize}
        %\item Assume Gaussian approximate posterior, $q(\fV) = \gaussianDist{\fV}{\muV}{\Cmat}$.
        %\item True posterior using Bayes rule, ${\color{intract}p(\fV|\yV)} = \frac{p(\yV|\fV)p(\fV)}{{\color{intract}p(\yV)}}$.
        %\item Cannot compute the KL divergence as we cannot compute the true posterior, $p(\fV|\yV)$.
    %\end{itemize}
    %\begin{align*}
        %\color{intract}\KL{q(\fV)}{p(\fV|\yV)} &= \expectationDist{\log \frac{q(\fV)}{{\color{intract}p(\fV|\yV)}}}{q(\fV)}\\
                                               %%&= \expectationDist{\log \frac{q(\fV)}{p(\fV)}p(\yV|\fV)p(\yV)}{q(\fV)}\\
                                               %&= \expectationDist{\log \frac{q(\fV)}{p(\fV)} - \log p(\yV|\fV) + {\color{intract}\log p(\yV)}}{q(\fV)}\\
                                               %&= \KL{q(\fV)}{p(\fV)} - \expectationDist{\log p(\yV|\fV)}{q(\fV)} + {\color{intract}\log p(\yV)}\\
        %{\color{intract}\log p(\yV)} &= {\color{tract}\expectationDist{\log p(\yV|\fV)}{q(\fV)}} - {\color{tract}\KL{q(\fV)}{p(\fV)}} + {\color{intract}\KL{q(\fV)}{p(\fV|\yV)}}
    %\end{align*}
%}

\frame{
    \frametitle{Variational Bayes for Gaussian processes}
    \begin{itemize}
        \item Make a Gaussian approximation, $q(\fV) = \gaussianDist{\fV}{\muV}{\Cmat}$, as similar possible to true posterior, ${\color{intract}p(\fV|\yV)}$.
        \item Treat $\muV$ and $\Cmat$ as `variational parameters', effecting quality of approximation.
        %\item True posterior using Bayes rule, ${\color{intract}p(\fV|\yV)} = \frac{p(\yV|\fV)p(\fV)}{{\color{intract}p(\yV)}}$.
        %\item Cannot compute the KL divergence as we cannot compute the true posterior, $p(\fV|\yV)$.
    \end{itemize}
    \begin{align*}
        \color{intract}\KL{q(\fV)}{p(\fV|\yV)} &= \expectationDist{\log \frac{q(\fV)}{{\color{intract}p(\fV|\yV)}}}{q(\fV)}\\
                                               %&= \expectationDist{\log \frac{q(\fV)}{p(\fV)}p(\yV|\fV)p(\yV)}{q(\fV)}\\
                                               &= \expectationDist{\log \frac{q(\fV)}{p(\fV)} - \log p(\yV|\fV) + {\color{intract}\log p(\yV)}}{q(\fV)}\\
                                               &= \KL{q(\fV)}{p(\fV)} - \expectationDist{\log p(\yV|\fV)}{q(\fV)} + {\color{intract}\log p(\yV)}\\
        {\color{intract}\log p(\yV)} &= {\color{tract}\expectationDist{\log p(\yV|\fV)}{q(\fV)}} - {\color{tract}\KL{q(\fV)}{p(\fV)}} + {\color{intract}\KL{q(\fV)}{p(\fV|\yV)}}
    \end{align*}
}

\frame{
    \frametitle{Variational Bayes for Gaussian processes - bound}
    \begin{align*} 
        {\color{intract}\log p(\yV)} &= {\color{tract}\expectationDist{\log p(\yV|\fV)}{q(\fV)}} - {\color{tract}\KL{q(\fV)}{p(\fV)}} + {\color{intract}\KL{q(\fV)}{p(\fV|\yV)}}\\
        &\geq {\color{tract}\expectationDist{\log p(\yV|\fV)}{q(\fV)}} - {\color{tract}\KL{q(\fV)}{p(\fV)}}
    \end{align*}
    \begin{itemize}
        \item Adjust variational parameters $\muV$ and $\Cmat$ to make tractable terms as large as possible, thus $\color{intract}\KL{q(\fV)}{p(\fV|\yV)}$ as small as possible.
        \item ${\color{tract}\expectationDist{\log p(\yV|\fV)}{q(\fV)}}$ with factorizing likelihood can be done with a series of $\numData$ 1 dimensional integrals.
        \item In practice, can reduce the number of variational parameters by reparameterizing $\Cmat = (\Kff - 2\Lambda)^{-1}$ by noting that the bound is constant in off diagonal terms of $\Cmat$.
    \end{itemize}
}

\frame{
    \frametitle{VB optimisation illustration for Gaussian processes }
    \animategraphics[loop,width=1\linewidth]{20}{../diagrams/animations/VGP-classification-frame-}{0}{237}
}

\subsection{Expectation propagation}
\frame{
\frametitle{Outline}
    \tableofcontents[currentsection,
    currentsubsection,
    subsectionstyle=show/shaded]
}

\frame{
    \frametitle{Expectation propagation}
    \begin{align*}
        {\color{red}p(\fV|\yV)} &\propto p(\fV)\prod^{\numData}_{i=1}p(\yV_{i}|\fV_{i})\\
        q(\fV) &\triangleq \frac{1}{Z_{ep}}p(\fV)\prod^{\numData}_{i=1}t_{i}(\fV_{i}|\tilde{Z}_{i}, \tilde{\mu}_{i}, \tilde{\sigma}^{2}_{i}) = \gaussianDist{\fV}{\muV}{\Sigma}\\
        t_{i} &\triangleq \tilde{Z}_{i} \gaussianDist{\fV_{i}}{\tilde{\muV}_{i}}{\tilde{\sigmaV}^{2}_{i}}
    \end{align*}
    \begin{itemize}
        \item Individual likelihood terms, $p(\yV_{i}|\fV_{i})$, replaced by independent un-normalised 1D Gaussians, $t_{i}$.
        \item Uses an iterative algorithm to update $t_{i}$'s, to get more and more accurate approximation.
        %such that the approximate marginal moments of $q(\fV_{i})$, match the marginal moments if $t_{i}$ was replaced with the true likelihood $p(\yV_{i}|\fV_{i})$. 
        %\item Visit each $i$ and find $t_{i}$ such that the approximate marginal moments of $q(\fV_{i}) = \int p(\fV)\prod^{n}_{j=1}t_{i}(\fV_{i}|\tilde{Z}_{i}, \tilde{\mu}_{i}, \tilde{\sigma}^{2}_{i})\,d\fV_{k \neq i}$ agree with marginals of $\int p(\fV)p(\yV_{i}|\fV_{i})\prod^{n}_{j \neq i}t_{i}(\fV_{i}|\tilde{Z}_{i}, \tilde{\mu}_{i}, \tilde{\sigma}^{2}_{i})\,d\fV_{k \neq i}$
    \end{itemize}
}

\frame{
    \frametitle{Expectation propagation}
    \begin{enumerate}
        \uncover<+->{\item Remove one factor $t_{i}$ from the approximation $q(\fV)$.}
        \uncover<+->{\item The approximate marginal $q(\fV_{i})$ with $t_{i}$ contribution removed is called cavity distribution, $q_{-i}(\fV_{i})$}
        %\uncover<+->{\item Minimize $\KL{p(\yV_{i}|\fV_{i})q_{-i}(\fV_{i})}{\gaussianDist{\fV_{i}}{\hat{\muV_{i}}}{\hat{\sigmaV}^{2}_{i}}\hat{Z}_{i}}$ by matching moments.}
        \uncover<+->{\item Find $t_{i}$ that minimises $\KL{p(\yV_{i}|\fV_{i})q_{-i}(\fV_{i})/z_{i}}{q(\fV_{i})}$ by matching moments.}
        %\uncover<+->{\item Update $t_{i}$ to share these moments and move onto another $i$}
        \uncover<+->{\item Repeat until convergence.}
    \end{enumerate}
    \uncover<+->{This approximately minimises $\KL{p(\fV|\yV)}{q(\fV)}$ locally, but not globally.}
}

\frame{
    \frametitle{Expectation propagation - in math}
    \uncover<+->{Step 1 \& 2. First choose a local likelihood contribution, $i$, to leave out, and find the marginal cavity distribution,
    \begin{align*}
        q(\fV|\yV) \propto p(\fV)\prod^{\numData}_{j=1}t_{j}(\fV_{j}) 
        &\rightarrow \frac{p(\fV)\prod^{\numData}_{j=1}t_{j}(\fV_{j})}{t_{i}(\fV_{i})} 
        \rightarrow p(\fV)\prod^{\numData}_{j \neq i}t_{j}(\fV_{j})\\
        &\rightarrow \int p(\fV)\prod_{j \neq i}t_{j}(\fV_{j})\,d\fV_{j \neq i}\triangleq q_{-i}(\fV_{i})
    \end{align*}}
    %Step 3.  $p(\yV_{i}|\fV_{i})q_{-i}(\fV_{i}) \triangleq \hat{q}(\fV_{i})$

    \uncover<+->{Step 3.1.  $\hat{q}(\fV_{i}) \approx \min \KL{p(\yV_{i}|\fV_{i})q_{-i}(\fV_{i})}{\gaussianDist{\fV_{i}}{\hat{\muV_{i}}}{\hat{\sigmaV}^{2}_{i}}\hat{Z}_{i}}$}
    
    \uncover<+->{Step 3.2: Compute parameters of $t_{i}(\fV_{i}|\tilde{Z}_{i}, \tilde{\mu}_{i}, \tilde{\sigma}^{2}_{i})$ making moments of $q(\fV_{i})$ match those of $\hat{Z}_{i}\gaussianDist{\fV_{i}}{\hat{\muV_{i}}}{\hat{\sigmaV}^{2}_{i}}$.}
}
\note{Write full unnormalized posterior down, marginalize all $j \neq i$. Calculate moments of left hand side. Find new $t_{i}$ which matches these moments for this marginal}


%\frame{
    %\frametitle{Expectation propagation}
    %\begin{enumerate}
        %\uncover<+->{\item From the approximate current posterior, $q(\fV|\yV)$, leave out one of the local likelihoods, $t_{i}$, and marginalise $\fV_{j}$ where $j \neq i$, giving rise to the approximate marginal with the contribution of one data removed; known as the \emph{cavity distribution}, $q_{-i}(\fV_{i})$.}
        %\uncover<+->{\item Combine resulting cavity distribution, $q_{-i}(\fV_{i})$, with exact likelihood contribution, $p(\yV_{i}|\fV_{i})$, giving a non-Gaussian un-normalized distribution, $\hat{q}(\fV_{i}) \triangleq p(\yV_{i}|\fV_{i})q_{-i}(\fV_{i})$.}
        %\uncover<+->{\item Choose a un-normalized Gaussian approximation to this distribution, $\gaussianDist{\fV_{i}}{\hat{\muV_{i}}}{\hat{\sigmaV}^{2}_{i}}\hat{Z}_{i}$, by finding moments of $\hat{q}(\fV_{i})$.}
        %%, thus minimizing $\KL{p(\yV_{i}|\fV_{i})q_{-i}(\fV_{i})}{\gaussianDist{\fV_{i}}{\hat{\muV_{i}}}{\hat{\sigmaV}^{2}_{i}}}$
        %\uncover<+->{\item Replace parameters of $t_{i}$ with those that produce the same moments as this approximation.}
        %\uncover<+->{\item This minimizes $\KL{p(\yV_{i}|\fV_{i})q_{-i}(\fV_{i})}{\gaussianDist{\fV_{i}}{\hat{\muV_{i}}}{\hat{\sigmaV}^{2}_{i}}\hat{Z}_{i}}$}
        %\uncover<+->{\item Choose another $i$ and start again. Repeat to convergence.}
    %\end{enumerate}
%}

%\frame{
    %\frametitle{Expectation propagation - in math}
    %Step 1. First choose a local likelihood contribution, $i$, to leave out, and find the marginal cavity distribution,
    %\begin{align*}
        %q(\fV|\yV) \propto p(\fV)\prod^{\numData}_{j=1}t_{j}(\fV_{j}) 
        %&\rightarrow \frac{p(\fV)\prod^{\numData}_{j=1}t_{j}(\fV_{j})}{t_{i}(\fV_{i})} 
        %\rightarrow p(\fV)\prod^{\numData}_{j \neq i}t_{j}(\fV_{j})\\
        %&\rightarrow \int p(\fV)\prod_{j \neq i}t_{j}(\fV_{j})\,d\fV_{j \neq i}\triangleq q_{-i}(\fV_{i})
    %\end{align*}
    %Step 2.  $p(\yV_{i}|\fV_{i})q_{-i}(\fV_{i}) \triangleq \hat{q}(\fV_{i})$

    %Step 3.  $\hat{q}(\fV_{i}) \approx \gaussianDist{\fV_{i}}{\hat{\muV_{i}}}{\hat{\sigmaV}^{2}_{i}}\hat{Z}_{i}$
    
    %Step 4: Compute parameters of $t_{i}(\fV_{i}|\tilde{Z}_{i}, \tilde{\mu}_{i}, \tilde{\sigma}^{2}_{i})$ making moments of $q(\fV_{i})$ match those of $\hat{Z}_{i}\gaussianDist{\fV_{i}}{\hat{\muV_{i}}}{\hat{\sigmaV}^{2}_{i}}$.

%}
%\note{Write full unnormalized posterior down, marginalize all $j \neq i$. Calculate moments of left hand side. Find new $t_{i}$ which matches these moments for this marginal}

%\frame{
    %\frametitle{Expectation propgation - illustrated}
    %ILLUSTRATION OF EP. Titlted distribution, cavity distribution, etc.
%}

\subsection{Comparisons}
\frame{
\frametitle{Outline}
    \tableofcontents[currentsection,
    currentsubsection,
    subsectionstyle=show/shaded]
}

\frame{
    \frametitle{Comparing posterior approximations}
    \begin{overprint}
        \onslide<1>\centerline{
            \includegraphics[width=0.55\textwidth]{../diagrams/joint_prior.pdf}
            \includegraphics[width=0.55\textwidth]{../diagrams/joint_likelihood.pdf}
        }
        \onslide<2>\centerline{
            \includegraphics[width=0.55\textwidth]{../diagrams/joint_posterior.pdf}
            \includegraphics[width=0.55\textwidth]{../diagrams/joint_laplace.pdf}
        }
        \onslide<3>\centerline{
            \includegraphics[width=0.55\textwidth]{../diagrams/joint_posterior.pdf}
            \includegraphics[width=0.55\textwidth]{../diagrams/joint_kl.pdf}
        }
        \onslide<4>\centerline{
            \includegraphics[width=0.55\textwidth]{../diagrams/joint_posterior.pdf}
            \includegraphics[width=0.55\textwidth]{../diagrams/joint_ep.pdf}
        }
    \end{overprint}
    \begin{overprint}
        \onslide<1> \begin{itemize}
            \item Gaussian prior between two function values $\{\fV_{1}, \fV_{2}\}$, at $\{\xV_{1}, \xV_{2}\}$ respectively. 
            \item Bernoulli likelihood, $\yV_{1} = 1$ and $\yV_{2} = 1$.\end{itemize}
        \onslide<2> \begin{itemize}
            \item $p(\fV|\yV) \propto \frac{p(\yV|\fV)p(\fV)}{p(\yV)}$
            \item True posterior is non-Gaussian.
            \item Laplace approximates with a Gaussian at the mode of the posterior.
        \end{itemize}
        \onslide<3> \begin{itemize}
            \item True posterior is non-Gaussian. 
            \item VB approximate with a Gaussian that has minimal KL divergence, $\KL{q(\fV)}{p(\fV|\yV)}$. 
            \item This leads to distributions that avoid regions in which $p(\fV|\yV)$ is small. 
            \item It has a large penality for assigning density where there is none.
        \end{itemize}
        \onslide<4> \begin{itemize}
            \item True posterior is non-Gaussian. 
            \item EP tends to try and put density where $p(\fV|\yV)$ is large
            \item Cares less about assigning density density where there is none. Contrasts to VB method.
        \end{itemize}
    \end{overprint}
}

\frame{
    \frametitle{Comparing posterior marginal approximations}
        \hspace{0.2em}
        \centerline{\includegraphics[width=0.6\textwidth,trim={0 1.7cm 0.5cm 0},clip]{../diagrams/marginals.pdf}}
        \hspace{-4em}
        \begin{itemize}
            \item Laplace: Poor approximation.
            \item VB: Avoids assigning density to areas where there is none, at the expense of areas where there is some (right tail).
            \item EP: Assigns density to areas with density, at the expense of areas where there is none (left tail).
        \end{itemize}
}
\frame{
    \frametitle{Pros - Cons - When - Laplace}
    Laplace approximation 
    \begin{itemize}
        \item Pros \begin{itemize}
                \item Simple to implement.
                \item Fast.
            \end{itemize}
        \item Cons \begin{itemize}
                \item Poor approximation if the mode does not well describe the posterior, for example Bernoulli likelihood.
            \end{itemize}
        \item When \begin{itemize}
                \item When the posterior \emph{is} well characterized by its mode, for example Poisson.
            \end{itemize}
    \end{itemize}
}

\frame{
    \frametitle{Pros - Cons - When - VB}
    Variational Bayes
    \begin{itemize}
        \item Pros \begin{itemize}
                \item Principled in that it we are directly optimizing a measure of divergence between an approximation and true distribution.
                \item Lends itself to sparse extensions.
                %\item Can be relatively quick, and lends it self to sparse approximations~\citep{Hensman:class15}.
            \end{itemize}
        \item Cons \begin{itemize}
                \item Requires factorizing likelihoods to avoid $\numData$ dimensional integral.
                \item As seen, can result in underestimating the variance, i.e.\ becomes overconfident.
                %\item Often requires numerical quadrature as part of inference, though in practice this can be very accurate.
            \end{itemize}
        \item When \begin{itemize}
                \item Applicable to a range of likelihood
                \item Might need to be careful if you wish to be conservative with predictive uncertainty.
            \end{itemize}
    \end{itemize}
}

\frame{
    \frametitle{Pros - Cons - When - EP}
    EP method
    \begin{itemize}
        \item Pros \begin{itemize}
                \item Very effective for certain likelihoods (classification).
                \item Also lends itself to sparse approximations.
            \end{itemize}
        \item Cons \begin{itemize}
                \item Standard algorithm is slow; though possible to extend to sparse case. 
                \item Not always guaranteed to converge.
                \item Can be brittle with initialisation and tricky implement.
            \end{itemize}
        \item When \begin{itemize}
                \item Binary data~\citep{Nickisch:class08,Kuss:robust06}, perhaps with truncated likelihood (censored data)~\citep{Vanhatalo:gpstuff15}.
                \item In conjunction with sparse methods.
            \end{itemize}
    \end{itemize}
}

\frame{
    \frametitle{Pros - Cons - When - MCMC}
    MCMC methods
    \begin{itemize}
        \item Pros \begin{itemize}
                \item Theoretical limit gives true distribution.
            \end{itemize}
        \item Cons \begin{itemize}
                \item Can be very slow.
            \end{itemize}
        \item When \begin{itemize}
                \item If time is not an issue, but exact accuracy is.
                \item If you are unsure whether a different approximation is appropriate, can be used as a ``ground truth''
            \end{itemize}
    \end{itemize}
}

\frame{
    \frametitle{Conclusion}
    \begin{itemize}
        \item Many real world tasks require non-Gaussian observation models.
        \item Non-Gaussian likelihoods cause complications in applying our framework.
        \item Several different ways to deal with the problem. Many are based on Gaussian approximations.
        \item Different methods have their own advantages and disadvantages.
    \end{itemize}
}

\frame{
    \frametitle{Questions}
    \begin{center}
        {\large Thanks for listening.\\
            \vspace{2em}Any questions?\\
        }
        \vspace{4em}
    \end{center}
}

\frame{
    \frametitle{Bonus - Hetroscedastic likelihoods}
    \centerline{\includegraphics[width=0.7\textwidth]{../diagrams/silverman_het_gauss.png}}
    \begin{itemize}
        \item Likelihood whos parameters are governed by two known functions, $\fV$ and $\mathbf{g}$.
        \item $p(\yV|\fV, \mathbf{g}) = \gaussianDist{\yV}{\mu=\fV}{\sigma^{2}=\exp(\mathbf{g})}$
    \end{itemize}
}

\frame{
    \frametitle{Bonus - non-Gaussian hetroscedastic likelihoods}
    \begin{center}
    \resizebox{1.0\textwidth}{!}{%
        \input{../diagrams/corrupt_motorcycle_all.tikz}
    }
    \end{center}
    \begin{itemize}
        \item Likelihood whos parameters are governed by two known functions, $\fV$ and $\mathbf{g}$.
        \item $p(\yV|\fV, \mathbf{g}) = t(\yV|\mu=\fV, \sigma^{2}=\exp(\mathbf{g}), \nu=3.0)$
    \end{itemize}
}
%\caption{Corrupted motorcycle dataset, fitted with a Gaussian
%process model with a Gaussian likelihood, a Gaussian process
%with input-dependent noise (heteroscedastic) with a Gaussian
%likelihood, and a Gaussian process with Student-$t$ likelihood,
%with an input-dependent shape parameter. The mean is
%shown in solid and the variance is shown as dotted}

\frame{
    \frametitle{Bonus - non-Gaussian hetroscedastic likelihoods}
    \begin{center}
        \includegraphics[width=0.33\textwidth,keepaspectratio]{../diagrams/crime_func_f_fast.pdf}
        \includegraphics[width=0.33\textwidth,keepaspectratio]{../diagrams/crime_func_g_slow.pdf}\\
        \includegraphics[width=0.33\textwidth,keepaspectratio]{../diagrams/time_f_fig.pdf}
        \includegraphics[width=0.33\textwidth,keepaspectratio]{../diagrams/time_g_fig.pdf}\\
    \end{center}
    \begin{itemize}
        %\item Rate parameter of Poisson as sum of spatial GP and temporal GP.
        \item $\Lambda(\xV, \mathbf{t}) = \lambda_1(\xV)\mu_1(\mathbf{t}) + \lambda_2(\xV)\mu_2(\mathbf{t})$
    \end{itemize}
}
%Homicide rate maps for Chicago. The short length scale spatial process, $\lambda_1(x)$ (above-left) is multiplied in the model by a temporal process, $\mu_1(t)$ (below-left) which fluctuates with passing seasons. Contours of spatial process are plotted as deaths per month per zip code area. Error bars on temporal processes are at 5th and 95th percentile. The longer length scale spatial process, $\lambda_2(x)$ (above-right) has been modeled with little to no fluctuation temporally $\mu_2(t)$ (below-right)

\frame[allowframebreaks]{%
    \frametitle{References}
    {\small
    \bibliography{lawrence,other,zbooks,../library}
    %\bibliography{lawrence.bib,other.bib,zbooks.bib,../library.bib}
    }
}

\end{document}
